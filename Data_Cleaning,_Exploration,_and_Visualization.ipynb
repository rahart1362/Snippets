{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMF4RTAPgZYYix6KnFQ4Q5Y"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install Required Libraries**\n",
        "\n",
        "  Ensure you install the necessary libraries using pip before proceeding with your code:\n",
        "\n",
        "*  pip install pandas openpyxl sqlalchemy matplotlib seaborn plotly gspread oauth2client"
      ],
      "metadata": {
        "id": "b5EYODZVexXV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import and Configure Libraries**\n",
        "\n",
        "  Below is the code to configure and use the libraries for working with Excel files, Google Drive, SQL databases, and data visualization:"
      ],
      "metadata": {
        "id": "XUBkL-WUfUo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries for data handling, visualization, and database operations:\n",
        "# - os: For operating system interactions (e.g., file and directory management).\n",
        "# - pandas (pd): For data manipulation and analysis using DataFrames.\n",
        "# - openpyxl: For reading/writing Excel (.xlsx) files.\n",
        "# - numpy (np): For numerical computations with arrays.\n",
        "# - matplotlib.pyplot (plt): For creating static and interactive visualizations.\n",
        "# - seaborn (sns): For statistical data visualizations built on Matplotlib.\n",
        "# - plotly.express (px): For interactive and dynamic visualizations.\n",
        "# - create_engine (SQLAlchemy): For establishing database connections.\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import openpyxl  # Import openpyxl explicitly\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from sqlalchemy import create_engine"
      ],
      "metadata": {
        "id": "fLm02OaffeMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reading Excel File from Windows Filesystem**"
      ],
      "metadata": {
        "id": "--I_BHf4hplk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File handling and data preview:\n",
        "# - Specify the file path to locate the Excel file.\n",
        "# - Read the Excel file into a Pandas DataFrame using the specified engine ('openpyxl').\n",
        "# - Display the first few rows of the DataFrame for a quick preview using .head().\n",
        "\n",
        "'''\n",
        "# Specify the file path\n",
        "file_path = \"C:/Windows/User/Desktop/SampleFile.xlsx\"\n",
        "\n",
        "# Read the Excel file\n",
        "# - file_path: The path to the Excel file\n",
        "# - engine='openpyxl': Specifies the engine to use for reading .xlsx files\n",
        "df_local = pd.read_excel(file_path, engine='openpyxl')\n",
        "\n",
        "# Display the first 5 rows (Default Argument\n",
        "print(df_local.head())\n",
        "\n",
        "# Argument: n=10 (shows the first 10 rows)\n",
        "print(df_local.head(10))\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "W-RCAB6thp3k",
        "outputId": "31b93211-92d5-4db1-b2dc-f09f470f1f60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Specify the file path\\nfile_path = \"C:/Windows/User/Desktop/SampleFile.xlsx\"\\n\\n# Read the Excel file\\ndf_local = pd.read_excel(file_path, engine=\\'openpyxl\\')\\n\\n# Display the first few rows\\nprint(df_local.head())\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query Rows in SQL DB using pyodbc or sqlalchemy**"
      ],
      "metadata": {
        "id": "j3eT4LzZmpcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import pyodbc\n",
        "\n",
        "# Define your connection parameters\n",
        "server = 'host.domain.subdomain.com,port'\n",
        "database = 'your_database_name'\n",
        "\n",
        "# Create the connection string\n",
        "connection_string = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};Trusted_Connection=yes;'\n",
        "\n",
        "# Establish the connection\n",
        "connection = pyodbc.connect(connection_string)\n",
        "print(\"Connection successful!\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define the SQL query\n",
        "query = \"SELECT TOP 10 * FROM SN_Data\"\n",
        "\n",
        "# Execute the query and load the results into a DataFrame\n",
        "df_testframe = pd.read_sql(query, connection)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df_testframe)\n",
        "\n",
        "###############\n",
        "\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# Define your connection parameters\n",
        "server = 'host.domain.subdomain.com,port'\n",
        "database = 'your_database_name'\n",
        "\n",
        "# Create the connection string\n",
        "connection_string = f'mssql+pyodbc://{server}/{database}?trusted_connection=yes&driver=ODBC+Driver+17+for+SQL+Server'\n",
        "\n",
        "# Create an SQLAlchemy engine\n",
        "engine = create_engine(connection_string)\n",
        "\n",
        "# Establish the connection\n",
        "connection = engine.connect()\n",
        "print(\"Connection successful!\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define the SQL query\n",
        "query = \"SELECT TOP 10 * FROM SN_Data\"\n",
        "\n",
        "# Execute the query and load the results into a DataFrame\n",
        "df_testframe = pd.read_sql(query, connection)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df_testframe)\n",
        "'''"
      ],
      "metadata": {
        "id": "F5Hk61dzmtpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Table and Dataframe Joins"
      ],
      "metadata": {
        "id": "TztFOwdLo5m4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "SQL Joins\n",
        "In SQL, joins are used to combine rows from two or more tables based on a related column. Here are the different types of joins:\n",
        "\n",
        "INNER JOIN: Returns only the rows with matching values in both tables.\n",
        "\n",
        "\n",
        "SELECT *\n",
        "FROM SQLTest_Table1\n",
        "INNER JOIN SQLTest_Table2\n",
        "ON SQLTest_Table1.common_column = SQLTest_Table2.common_column;\n",
        "Use when: You want to retrieve records that have matching values in both tables.\n",
        "LEFT JOIN (or LEFT OUTER JOIN): Returns all rows from the left table and the matched rows from the right table. If no match is found, NULL values are returned for columns from the right table.\n",
        "\n",
        "\n",
        "SELECT *\n",
        "FROM SQLTest_Table1\n",
        "LEFT JOIN SQLTest_Table2\n",
        "ON SQLTest_Table1.common_column = SQLTest_Table2.common_column;\n",
        "Use when: You want to retrieve all records from the left table and the matched records from the right table.\n",
        "RIGHT JOIN (or RIGHT OUTER JOIN): Returns all rows from the right table and the matched rows from the left table. If no match is found, NULL values are returned for columns from the left table.\n",
        "\n",
        "\n",
        "SELECT *\n",
        "FROM SQLTest_Table1\n",
        "RIGHT JOIN SQLTest_Table2\n",
        "ON SQLTest_Table1.common_column = SQLTest_Table2.common_column;\n",
        "Use when: You want to retrieve all records from the right table and the matched records from the left table.\n",
        "FULL JOIN (or FULL OUTER JOIN): Returns rows when there is a match in one of the tables. It returns all rows from both tables, with NULLs in places where the match is not found.\n",
        "\n",
        "\n",
        "SELECT *\n",
        "FROM SQLTest_Table1\n",
        "FULL JOIN SQLTest_Table2\n",
        "ON SQLTest_Table1.common_column = SQLTest_Table2.common_column;\n",
        "Use when: You want to retrieve all records when there is a match in either left or right table records.\n",
        "Pandas Joins\n",
        "In pandas, you can use the merge function to perform joins on dataframes. Here are the different types of joins:\n",
        "\n",
        "Inner Join: Returns only the rows with matching values in both dataframes.\n",
        "\n",
        "\n",
        "result = pd.merge(df_testframe1, df_testframe2, on='common_column', how='inner')\n",
        "Use when: You want to retrieve records that have matching values in both dataframes.\n",
        "Left Join: Returns all rows from the left dataframe and the matched rows from the right dataframe. If no match is found, NaN values are returned for columns from the right dataframe.\n",
        "\n",
        "\n",
        "result = pd.merge(df_testframe1, df_testframe2, on='common_column', how='left')\n",
        "Use when: You want to retrieve all records from the left dataframe and the matched records from the right dataframe.\n",
        "Right Join: Returns all rows from the right dataframe and the matched rows from the left dataframe. If no match is found, NaN values are returned for columns from the left dataframe.\n",
        "\n",
        "\n",
        "result = pd.merge(df_testframe1, df_testframe2, on='common_column', how='right')\n",
        "Use when: You want to retrieve all records from the right dataframe and the matched records from the left dataframe.\n",
        "Outer Join: Returns all rows from both dataframes, with NaN values in places where the match is not found.\n",
        "\n",
        "\n",
        "result = pd.merge(df_testframe1, df_testframe2, on='common_column', how='outer')\n",
        "Use when: You want to retrieve all records when there is a match in either left or right dataframe records.\n",
        "'''"
      ],
      "metadata": {
        "id": "uBrmHEOjpTBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query Specific Columns in SQL Table**"
      ],
      "metadata": {
        "id": "f0qhx3_Cpu9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "For SQL Server:\n",
        "\n",
        "SELECT TOP 10 Column1, Column2, Column3\n",
        "FROM SQLTest_Table;\n",
        "For MySQL, PostgreSQL, SQLite, etc.:\n",
        "\n",
        "SELECT Column1, Column2, Column3\n",
        "FROM SQLTest_Table\n",
        "LIMIT 10;\n",
        "These queries will return the first 10 rows of the specified columns from the SQLTest_Table. If you need to execute this query in a Jupyter notebook, you can use a library like pandas along with a database connector (e.g., sqlalchemy, pyodbc) to run the SQL query and fetch the results into a DataFrame. Here's an example using pandas and sqlalchemy:\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# Create a connection to the database\n",
        "engine = create_engine('your_database_connection_string')\n",
        "\n",
        "# Define the SQL query\n",
        "query = \"SELECT Column1, Column2, Column3 FROM SQLTest_Table LIMIT 10;\"\n",
        "\n",
        "# Execute the query and load the results into a DataFrame\n",
        "df = pd.read_sql(query, engine)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n",
        "'''"
      ],
      "metadata": {
        "id": "i2vrzftup3YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reading Excel File from Google Drive**"
      ],
      "metadata": {
        "id": "FTVvegKwh8QP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify the file path on Google Drive\n",
        "file_path = \"/content/drive/My Drive/Python Reference Sheets/Practice Data and Notebook/PracticeDataforPython.xlsx\"\n",
        "\n",
        "# Read the Excel file\n",
        "df_testframe = pd.read_excel(file_path, engine='openpyxl')\n",
        "\n",
        "# Display the first few rows\n",
        "print(df_testframe.head())\n",
        "\n",
        "#Display the data type of each column\n",
        "print(df_testframe.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2KeBM0Yh8ef",
        "outputId": "3d7974dd-3ea7-4c7a-b5e9-3139389919f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "                Date                 Name  Height (ft)  Weight (lbs)  \\\n",
            "0  December 17, 2024       Donna Gonzalez         5.78         189.4   \n",
            "1         2016-12-23      Spencer Edwards        54.00         173.0   \n",
            "2     March 23, 2024     Jonathan Anthony         5.98         148.0   \n",
            "3         2019-02-09  Alexander Butler MD        61.00         177.0   \n",
            "4     March 02, 2024   Christopher Harris         4.95         195.0   \n",
            "\n",
            "        State         City                 LatLong  \n",
            "0  New Mexico  Albuquerque  -79.931169, 115.848934  \n",
            "1    New York      Buffolo   54.908244, -35.580677  \n",
            "2     Georgia      Atlanta  -78.086051, 148.729005  \n",
            "3    Illinois      Chicago    12.092409, 78.449137  \n",
            "4      Nevada    Las Vegas   -51.727222, -0.276747  \n",
            "Date             object\n",
            "Name             object\n",
            "Height (ft)     float64\n",
            "Weight (lbs)    float64\n",
            "State            object\n",
            "City             object\n",
            "LatLong          object\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Data Types for each Column, standardize format, and identify outliers**"
      ],
      "metadata": {
        "id": "9BfSy4sati6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to standardize the date format\n",
        "def standardize_date(date_str):\n",
        "    # List of possible date formats\n",
        "    date_formats = [\n",
        "        '%B %d, %Y',  # March 12, 2003\n",
        "        '%m/%d/%Y',   # 03/12/2003\n",
        "        '%m/%d/%y',   # 3/12/03\n",
        "        '%-m/%-d/%Y', # 1/1/2004\n",
        "        '%-m/%-d/%y', # 01/01/04\n",
        "        '%B %d, %Y'   # January 1, 2004\n",
        "    ]\n",
        "\n",
        "    # Check if the date is already in YYYY-MM-DD format\n",
        "    if pd.to_datetime(date_str, errors='coerce', format='%Y-%m-%d') is not pd.NaT:\n",
        "        return date_str\n",
        "\n",
        "    # Try each format until one works\n",
        "    for date_format in date_formats:\n",
        "        try:\n",
        "            return pd.to_datetime(date_str, format=date_format).strftime('%Y-%m-%d')\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "    # If none of the formats work, return the original string\n",
        "    return date_str\n",
        "\n",
        "# Apply the function to the Date column\n",
        "df_testframe['Date'] = df_testframe['Date'].apply(standardize_date)\n",
        "\n",
        "# Convert the Date column to datetime\n",
        "df_testframe['Date'] = pd.to_datetime(df_testframe['Date'], errors='coerce')\n",
        "\n",
        "#print(df_testframe)"
      ],
      "metadata": {
        "id": "IlwRfZ2sLPLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Standardize Name Formats**"
      ],
      "metadata": {
        "id": "TGdWoGhzS6CQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to standardize name format\n",
        "def standardize_name(name):\n",
        "    # Check if the name is in the 'Last, First Middle' format\n",
        "    if ', ' in name:\n",
        "        parts = name.split(', ')\n",
        "        if len(parts) == 2:\n",
        "            last_name, first_middle = parts\n",
        "            first_middle_parts = first_middle.split()\n",
        "            if len(first_middle_parts) == 2:\n",
        "                first, middle = first_middle_parts\n",
        "                return f'{first} {middle} {last_name}'\n",
        "            elif len(first_middle_parts) == 3:\n",
        "                first, middle, suffix = first_middle_parts\n",
        "                return f'{first} {middle} {last_name} {suffix}'\n",
        "            else:\n",
        "                return f'{first_middle} {last_name}'\n",
        "        else:\n",
        "            return name\n",
        "    else:\n",
        "        return name\n",
        "\n",
        "# Apply the function to the Name column\n",
        "df_testframe['Name'] = df_testframe['Name'].apply(standardize_name)\n",
        "\n",
        "#print(df_testframe)"
      ],
      "metadata": {
        "id": "d3ulwjX9S_p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert Columns from Float to Integer**"
      ],
      "metadata": {
        "id": "DT1Q0btYcPce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Separate rows with NaN values in the Weight column and reset Index\n",
        "df_irregular = df_testframe[df_testframe['Weight (lbs)'].isna()].reset_index(drop=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Keep only rows with non-NaN values in the Weight column and reset Index\n",
        "df_testframe = df_testframe[df_testframe['Weight (lbs)'].notna()].reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Round the Weight column to the nearest whole number\n",
        "df_testframe['Weight (lbs)'] = df_testframe['Weight (lbs)'].round(0)\n",
        "\n",
        "# Handle NaN or infinite values by filling them with a default value (e.g., 0)\n",
        "# df_testframe['Weight (lbs)'] = df_testframe['Weight (lbs)'].fillna(0)\n",
        "\n",
        "\n",
        "\n",
        "# Convert the Weight column from float to integer\n",
        "df_testframe['Weight (lbs)'] = df_testframe['Weight (lbs)'].astype(int)\n",
        "\n",
        "print(\"Regular DataFrame (df_testframe):\")\n",
        "#print(df_testframe)\n",
        "\n",
        "print(\"\\nIrregular DataFrame (df_irregular):\")\n",
        "print(df_irregular)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeH55kpjc9Vf",
        "outputId": "5ab3c9f1-3328-4146-c212-410dd2258bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regular DataFrame (df_testframe):\n",
            "\n",
            "Irregular DataFrame (df_irregular):\n",
            "        Date             Name  Height (ft)  Weight (lbs)    State     City  \\\n",
            "0 2024-09-12  Angela Williams         5.68           NaN  Florida  Orlando   \n",
            "1 1987-09-04   Robert Johnson        60.00           NaN    Texas   Austin   \n",
            "\n",
            "                  LatLong  \n",
            "0  89.518640, -130.200572  \n",
            "1    79.637555, 64.902028  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Change column sequence and sort rows**"
      ],
      "metadata": {
        "id": "H6egvK_w8SxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resequence the columns\n",
        "column_order = ['Date', 'Name', 'Height (ft)', 'Weight (lbs)', 'City', 'State', 'LatLong']\n",
        "df_testframe = df_testframe.reindex(columns=column_order)\n",
        "df_irregular = df_irregular.reindex(columns=column_order)\n",
        "\n",
        "\n",
        "# Create a temporary column with the last name extracted from the Name column\n",
        "df_testframe['LastName'] = df_testframe['Name'].apply(lambda x: x.split()[-1])\n",
        "\n",
        "# Create a temporary column with the first 3 characters of the State column\n",
        "df_testframe['State_First3'] = df_testframe['State'].str[:3]\n",
        "\n",
        "# Sort df_testframe by the first 3 characters of the State column in Z-A order and then by LastName in A-Z order\n",
        "df_testframe = df_testframe.sort_values(by=['State_First3', 'LastName'], ascending=[False, True]).reset_index(drop=True)\n",
        "\n",
        "# Drop the temporary columns\n",
        "df_testframe = df_testframe.drop(columns=['State_First3', 'LastName'])\n",
        "\n",
        "\n",
        "#print(df_testframe)"
      ],
      "metadata": {
        "id": "ykV-rZAu8sKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explore the general quantity shape and distinct values of a dataframe**"
      ],
      "metadata": {
        "id": "z6qsGjN3J73H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Explore_Data(df):\n",
        "    print(\"First few rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "    print(\"\\nSummary statistics:\")\n",
        "    print(df.describe(include='all'))\n",
        "    print(\"\\nInformation about the DataFrame:\")\n",
        "    print(df.info())\n",
        "    print(\"\\nNumber of missing values in each column:\")\n",
        "    print(df.isnull().sum())\n",
        "    print(\"\\nUnique values in each column:\")\n",
        "    for column in df.columns:\n",
        "        print(f\"{column}: {df[column].nunique()} unique values\")\n",
        "    print(\"\\nValue counts for each column:\")\n",
        "    for column in df.columns:\n",
        "        print(f\"{column}:\")\n",
        "        print(df[column].value_counts(dropna=False))\n",
        "\n",
        "# Apply the Explore_Data function to df_testframe\n",
        "Explore_Data(df_testframe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2CJ9PCXKHiD",
        "outputId": "04b46163-10b2-4b44-df90-9191fcd2f5ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of the DataFrame:\n",
            "        Date             Name  Height (ft)  Weight (lbs)         City  \\\n",
            "0 2002-10-17    Vanessa Jones        77.00           217       Dallas   \n",
            "1 2024-10-29   Kristen Miller         4.56           155       Austin   \n",
            "2 2024-03-17    Shannon Boone         4.70           120     Santa Fe   \n",
            "3 2016-12-23  Spencer Edwards        54.00           173      Buffolo   \n",
            "4 2024-12-17   Donna Gonzalez         5.78           189  Albuquerque   \n",
            "\n",
            "        State                  LatLong  \n",
            "0       Texas     44.098017, 14.031824  \n",
            "1      Tex,as   64.998524, -124.977125  \n",
            "2  New Mexico  -64.283114, -129.733085  \n",
            "3    New York    54.908244, -35.580677  \n",
            "4  New Mexico   -79.931169, 115.848934  \n",
            "\n",
            "Summary statistics:\n",
            "                       Date           Name  Height (ft)  Weight (lbs)  \\\n",
            "count                    18             18    18.000000     18.000000   \n",
            "unique                  NaN             18          NaN           NaN   \n",
            "top                     NaN  Vanessa Jones          NaN           NaN   \n",
            "freq                    NaN              1          NaN           NaN   \n",
            "mean    2012-10-30 16:00:00            NaN    35.372222    167.055556   \n",
            "min     1972-08-20 00:00:00            NaN     4.550000    120.000000   \n",
            "25%     2006-10-09 18:00:00            NaN     5.172500    147.250000   \n",
            "50%     2021-03-08 00:00:00            NaN    30.140000    163.000000   \n",
            "75%     2024-03-13 06:00:00            NaN    60.750000    186.000000   \n",
            "max     2024-12-17 00:00:00            NaN    77.000000    217.000000   \n",
            "std                     NaN            NaN    31.712002     28.916660   \n",
            "\n",
            "            City       State               LatLong  \n",
            "count         18          18                    17  \n",
            "unique        13           9                    17  \n",
            "top     Santa Fe  New Mexico  44.098017, 14.031824  \n",
            "freq           3           4                     1  \n",
            "mean         NaN         NaN                   NaN  \n",
            "min          NaN         NaN                   NaN  \n",
            "25%          NaN         NaN                   NaN  \n",
            "50%          NaN         NaN                   NaN  \n",
            "75%          NaN         NaN                   NaN  \n",
            "max          NaN         NaN                   NaN  \n",
            "std          NaN         NaN                   NaN  \n",
            "\n",
            "Information about the DataFrame:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 18 entries, 0 to 17\n",
            "Data columns (total 7 columns):\n",
            " #   Column        Non-Null Count  Dtype         \n",
            "---  ------        --------------  -----         \n",
            " 0   Date          18 non-null     datetime64[ns]\n",
            " 1   Name          18 non-null     object        \n",
            " 2   Height (ft)   18 non-null     float64       \n",
            " 3   Weight (lbs)  18 non-null     int64         \n",
            " 4   City          18 non-null     object        \n",
            " 5   State         18 non-null     object        \n",
            " 6   LatLong       17 non-null     object        \n",
            "dtypes: datetime64[ns](1), float64(1), int64(1), object(4)\n",
            "memory usage: 1.1+ KB\n",
            "None\n",
            "\n",
            "Number of missing values in each column:\n",
            "Date            0\n",
            "Name            0\n",
            "Height (ft)     0\n",
            "Weight (lbs)    0\n",
            "City            0\n",
            "State           0\n",
            "LatLong         1\n",
            "dtype: int64\n",
            "\n",
            "Unique values in each column:\n",
            "Date: 18 unique values\n",
            "Name: 18 unique values\n",
            "Height (ft): 17 unique values\n",
            "Weight (lbs): 15 unique values\n",
            "City: 13 unique values\n",
            "State: 9 unique values\n",
            "LatLong: 17 unique values\n",
            "\n",
            "Value counts for each column:\n",
            "Date:\n",
            "Date\n",
            "2002-10-17    1\n",
            "2024-10-29    1\n",
            "2024-03-23    1\n",
            "2009-04-02    1\n",
            "1972-08-20    1\n",
            "2019-02-09    1\n",
            "2005-12-12    1\n",
            "2024-03-02    1\n",
            "1991-05-30    1\n",
            "2023-08-16    1\n",
            "2023-08-12    1\n",
            "2023-04-05    1\n",
            "2024-09-04    1\n",
            "2014-09-28    1\n",
            "2024-12-17    1\n",
            "2016-12-23    1\n",
            "2024-03-17    1\n",
            "1980-07-20    1\n",
            "Name: count, dtype: int64\n",
            "Name:\n",
            "Name\n",
            "Vanessa Jones          1\n",
            "Kristen Miller         1\n",
            "Jonathan Anthony       1\n",
            "Michael Anderson       1\n",
            "Scott Potter           1\n",
            "Alexander Butler MD    1\n",
            "Steven Jones           1\n",
            "Christopher Harris     1\n",
            "Alex Smith             1\n",
            "Jane Smith             1\n",
            "Robert Smith           1\n",
            "Bradley Lopez          1\n",
            "Julie Hernandez        1\n",
            "Cynthia Hebert         1\n",
            "Donna Gonzalez         1\n",
            "Spencer Edwards        1\n",
            "Shannon Boone          1\n",
            "Christopher Lawson     1\n",
            "Name: count, dtype: int64\n",
            "Height (ft):\n",
            "Height (ft)\n",
            "77.00    2\n",
            "76.00    1\n",
            "5.98     1\n",
            "60.00    1\n",
            "73.00    1\n",
            "61.00    1\n",
            "56.00    1\n",
            "4.95     1\n",
            "5.93     1\n",
            "4.56     1\n",
            "4.55     1\n",
            "4.97     1\n",
            "6.28     1\n",
            "5.78     1\n",
            "54.00    1\n",
            "4.70     1\n",
            "55.00    1\n",
            "Name: count, dtype: int64\n",
            "Weight (lbs):\n",
            "Weight (lbs)\n",
            "217    2\n",
            "155    2\n",
            "163    2\n",
            "120    1\n",
            "173    1\n",
            "189    1\n",
            "141    1\n",
            "147    1\n",
            "131    1\n",
            "168    1\n",
            "195    1\n",
            "209    1\n",
            "177    1\n",
            "139    1\n",
            "148    1\n",
            "Name: count, dtype: int64\n",
            "City:\n",
            "City\n",
            "Santa Fe         3\n",
            "Buffalo          2\n",
            "Chicago          2\n",
            "Atlanta          2\n",
            "Dallas           1\n",
            "Austin           1\n",
            "Buffolo          1\n",
            "Albuquerque      1\n",
            "New York City    1\n",
            "Trenton          1\n",
            "Las Vegas        1\n",
            "Springfield      1\n",
            "Los Angeles      1\n",
            "Name: count, dtype: int64\n",
            "State:\n",
            "State\n",
            "New Mexico    4\n",
            "New York      4\n",
            "Illinois      3\n",
            "Georgia       2\n",
            "Texas         1\n",
            "Tex,as        1\n",
            "New Jersey    1\n",
            "Nevada        1\n",
            "California    1\n",
            "Name: count, dtype: int64\n",
            "LatLong:\n",
            "LatLong\n",
            "44.098017, 14.031824       1\n",
            "64.998524, -124.977125     1\n",
            "-78.086051, 148.729005     1\n",
            "-61.203240, 64.973237      1\n",
            "17.259917, -11.502040      1\n",
            "12.092409, 78.449137       1\n",
            "15.035926, -49.681280      1\n",
            "-51.727222, -0.276747      1\n",
            "45.158004, 96.695544       1\n",
            "-69.380686, 138.523842     1\n",
            "NaN                        1\n",
            "17.353626, -41.483768      1\n",
            "44.462486, -25.763822      1\n",
            "69.242968, 51.426673       1\n",
            "-79.931169, 115.848934     1\n",
            "54.908244, -35.580677      1\n",
            "-64.283114, -129.733085    1\n",
            "-1.167150, 92.081577       1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perform basic calculations and Summary (Pivots) of data with categorization**"
      ],
      "metadata": {
        "id": "armAb6A6MNaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = {\n",
        "    'Date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04'],\n",
        "    'Name': ['John Doe', 'Jane Smith', 'Alice Johnson', 'Bob Brown'],\n",
        "    'Height (ft)': [5.9, 5.5, 5.7, 6.0],\n",
        "    'Weight (lbs)': [70.5, 65.2, 80.8, np.nan],  # Including a NaN value for demonstration\n",
        "    'State': ['California', 'New York', 'Texas', 'Florida'],\n",
        "    'City': ['Los Angeles', 'New York', 'Houston', 'Miami'],\n",
        "    'LatLong': ['34.0522,-118.2437', '40.7128,-74.0060', '29.7604,-95.3698', '25.7617,-80.1918']\n",
        "}\n",
        "\n",
        "# df_testframe = pd.DataFrame(data)\n",
        "\n",
        "# 1. Sum of the \"Weight (lbs)\" column\n",
        "total_weight = df_testframe['Weight (lbs)'].sum()\n",
        "print(f\"Total sum of 'Weight (lbs)': {total_weight}\")\n",
        "\n",
        "# 2. Sum of \"Weight (lbs)\" for \"State\" values matching \"Texas\" and subtract that from the sum of \"Weight (lbs)\" for \"State\" values matching \"New York\"\n",
        "weight_texas = df_testframe[df_testframe['State'] == 'Texas']['Weight (lbs)'].sum()\n",
        "weight_new_york = df_testframe[df_testframe['State'] == 'New York']['Weight (lbs)'].sum()\n",
        "difference = weight_new_york - weight_texas\n",
        "print(f\"Difference in 'Weight (lbs)' between New York and Texas: {difference}\")\n",
        "\n",
        "\n",
        "# Indicate which state had the greater value\n",
        "if difference > 0:\n",
        "    print(f\"New York has a greater sum of 'Weight (lbs)' by {difference} lbs compared to Texas.\")\n",
        "elif difference < 0:\n",
        "    print(f\"Texas has a greater sum of 'Weight (lbs)' by {-difference} lbs compared to New York.\")\n",
        "else:\n",
        "    print(\"The sum of 'Weight (lbs)' for Texas and New York is equal.\")\n",
        "\n",
        "\n",
        "\n",
        "# 3. Pivot table showing the sums of \"Weight (lbs)\" and \"Height (ft)\" grouped by \"State\" and \"City\"\n",
        "pivot_table = df_testframe.pivot_table(values=['Weight (lbs)', 'Height (ft)'], index=['State', 'City'], aggfunc='sum')\n",
        "print(\"Pivot table:\")\n",
        "print(pivot_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Se-VfPJMayL",
        "outputId": "b20cd19b-dd33-44e0-d2ce-6711bc63eb61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sum of 'Weight (lbs)': 3007\n",
            "Difference in 'Weight (lbs)' between New York and Texas: 389\n",
            "New York has a greater sum of 'Weight (lbs)' by 389 lbs compared to Texas.\n",
            "Pivot table:\n",
            "                          Height (ft)  Weight (lbs)\n",
            "State      City                                    \n",
            "California Los Angeles          55.00           163\n",
            "Georgia    Atlanta              65.98           287\n",
            "Illinois   Chicago             117.00           386\n",
            "           Springfield          73.00           217\n",
            "Nevada     Las Vegas             4.95           195\n",
            "New Jersey Trenton              76.00           168\n",
            "New Mexico Albuquerque           5.78           189\n",
            "           Santa Fe             15.53           424\n",
            "New York   Buffalo              81.97           302\n",
            "           Buffolo              54.00           173\n",
            "           New York City         5.93           131\n",
            "Tex,as     Austin                4.56           155\n",
            "Texas      Dallas               77.00           217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualization Explanation:**\n",
        "\n",
        "**Matplotlib Visualizations:**\n",
        "- Bar Plot of Weight by State: Shows the total weight by state.\n",
        "- Scatter Plot of Height vs. Weight: Shows the relationship between height and weight.\n",
        "- Pie Chart of State Distribution: Shows the distribution of states.\n",
        "\n",
        "**Seaborn Visualizations:**\n",
        "- Box Plot of Weight by State: Shows the distribution of weights within each state.\n",
        "- Violin Plot of Height by State: Shows the distribution of heights within each state.\n",
        "- Heatmap of Correlation Matrix: Shows the correlation matrix for numeric columns only.\n",
        "\n",
        "**Plotly Visualizations:**\n",
        "- Bar Plot of Weight by State: Shows the total weight by state.\n",
        "Scatter Plot of Height vs. Weight: Shows the relationship between height and weight.\n",
        "- Pie Chart of State Distribution: Shows the distribution of states.\n",
        "\n",
        "*By selecting only the numeric columns for the correlation matrix, the error is resolved, and the visualizations provide a comprehensive overview of the sample data.*"
      ],
      "metadata": {
        "id": "ZJkaYnNsZE6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import pandas as pd  # Import the pandas library for data manipulation\n",
        "import numpy as np  # Import numpy for numerical operations\n",
        "import matplotlib.pyplot as plt  # Import matplotlib's pyplot for creating visualizations\n",
        "import seaborn as sns  # Import seaborn for statistical data visualizations\n",
        "import plotly.express as px  # Import plotly.express for interactive plots\n",
        "import plotly.graph_objects as go  # Import plotly.graph_objects for advanced interactive plots\n",
        "\n",
        "# Sample data in a dictionary format\n",
        "data = {\n",
        "    'Date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04'],\n",
        "    'Name': ['John Doe', 'Jane Smith', 'Alice Johnson', 'Bob Brown'],\n",
        "    'Height (ft)': [5.9, 5.5, 5.7, 6.0],\n",
        "    'Weight (lbs)': [70.5, 65.2, 80.8, np.nan],  # Including a NaN value for demonstration\n",
        "    'State': ['California', 'New York', 'Texas', 'Florida'],\n",
        "    'City': ['Los Angeles', 'New York', 'Houston', 'Miami'],\n",
        "    'LatLong': ['34.0522,-118.2437', '40.7128,-74.0060', '29.7604,-95.3698', '25.7617,-80.1918']\n",
        "}\n",
        "\n",
        "# Create a pandas DataFrame from the sample data\n",
        "df_testframe = pd.DataFrame(data)\n",
        "\n",
        "# Matplotlib Visualizations\n",
        "\n",
        "# Bar Plot of Weight by State\n",
        "plt.figure(figsize=(10, 6))  # Create a new figure with specified size (width=10, height=6)\n",
        "df_testframe.groupby('State')['Weight (lbs)'].sum().plot(kind='bar', color='skyblue')\n",
        "# Group data by 'State', sum the 'Weight (lbs)' for each state, and plot as a bar chart\n",
        "# Arguments:\n",
        "# - kind='bar': Specifies a bar plot\n",
        "# - color='skyblue': Sets the color of the bars\n",
        "plt.title('Total Weight by State')  # Set the title of the plot\n",
        "plt.xlabel('State')  # Set the label for the x-axis\n",
        "plt.ylabel('Total Weight (lbs)')  # Set the label for the y-axis\n",
        "plt.xticks(rotation=45)  # Rotate the x-axis labels by 45 degrees for better readability\n",
        "plt.show()  # Display the plot\n",
        "\n",
        "# Scatter Plot of Height vs. Weight\n",
        "plt.figure(figsize=(10, 6))  # Create a new figure with specified size (width=10, height=6)\n",
        "plt.scatter(df_testframe['Height (ft)'], df_testframe['Weight (lbs)'], color='green')\n",
        "# Create a scatter plot for 'Height (ft)' vs. 'Weight (lbs)'\n",
        "# Arguments:\n",
        "# - color='green': Sets the color of the points\n",
        "plt.title('Height vs. Weight')  # Set the title of the plot\n",
        "plt.xlabel('Height (ft)')  # Set the label for the x-axis\n",
        "plt.ylabel('Weight (lbs)')  # Set the label for the y-axis\n",
        "plt.show()  # Display the plot\n",
        "\n",
        "# Pie Chart of State Distribution\n",
        "plt.figure(figsize=(8, 8))  # Create a new figure with specified size (8x8 inches)\n",
        "df_testframe['State'].value_counts().plot(kind='pie', autopct='%1.1f%%', startangle=140, colors=['gold', 'yellowgreen', 'lightcoral', 'lightskyblue'])\n",
        "# Plot a pie chart for state distribution using value counts of the 'State' column\n",
        "# Arguments:\n",
        "# - kind='pie': Specifies a pie chart\n",
        "# - autopct='%1.1f%%': Shows percentage values on the chart\n",
        "# - startangle=140: Rotates the chart to start from a specific angle\n",
        "# - colors: List of colors to use for different segments of the pie\n",
        "plt.title('State Distribution')  # Set the title of the plot\n",
        "plt.ylabel('')  # Remove the y-axis label (default for pie charts)\n",
        "plt.show()  # Display the plot\n",
        "\n",
        "# Seaborn Visualizations\n",
        "\n",
        "# Box Plot of Weight by State\n",
        "plt.figure(figsize=(10, 6))  # Create a new figure with specified size (width=10, height=6)\n",
        "sns.boxplot(x='State', y='Weight (lbs)', data=df_testframe)\n",
        "# Create a box plot for 'Weight (lbs)' by 'State'\n",
        "# Arguments:\n",
        "# - x='State': Specifies the categorical variable (states)\n",
        "# - y='Weight (lbs)': Specifies the numeric variable (weight)\n",
        "# - data=df_testframe: Specifies the DataFrame to use\n",
        "plt.title('Weight Distribution by State')  # Set the title of the plot\n",
        "plt.xlabel('State')  # Set the label for the x-axis\n",
        "plt.ylabel('Weight (lbs)')  # Set the label for the y-axis\n",
        "plt.xticks(rotation=45)  # Rotate the x-axis labels by 45 degrees for readability\n",
        "plt.show()  # Display the plot\n",
        "\n",
        "# Violin Plot of Height by State\n",
        "plt.figure(figsize=(10, 6))  # Create a new figure with specified size (width=10, height=6)\n",
        "sns.violinplot(x='State', y='Height (ft)', data=df_testframe)\n",
        "# Create a violin plot for 'Height (ft)' by 'State'\n",
        "# Arguments:\n",
        "# - x='State': Specifies the categorical variable (states)\n",
        "# - y='Height (ft)': Specifies the numeric variable (height)\n",
        "# - data=df_testframe: Specifies the DataFrame to use\n",
        "plt.title('Height Distribution by State')  # Set the title of the plot\n",
        "plt.xlabel('State')  # Set the label for the x-axis\n",
        "plt.ylabel('Height (ft)')  # Set the label for the y-axis\n",
        "plt.xticks(rotation=45)  # Rotate the x-axis labels by 45 degrees for readability\n",
        "plt.show()  # Display the plot\n",
        "\n",
        "# Heatmap of Correlation Matrix (only for numeric columns)\n",
        "plt.figure(figsize=(10, 6))  # Create a new figure with specified size (width=10, height=6)\n",
        "numeric_columns = df_testframe.select_dtypes(include=[np.number]).columns  # Select numeric columns from the DataFrame\n",
        "correlation_matrix = df_testframe[numeric_columns].corr()  # Compute the correlation matrix for numeric columns\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "# Create a heatmap to visualize correlations between numeric variables\n",
        "# Arguments:\n",
        "# - annot=True: Annotates the heatmap with the correlation values\n",
        "# - cmap='coolwarm': Specifies the color map for the heatmap\n",
        "plt.title('Correlation Matrix')  # Set the title of the plot\n",
        "plt.show()  # Display the plot\n",
        "\n",
        "# Plotly Visualizations\n",
        "\n",
        "# Bar Plot of Weight by State\n",
        "fig = px.bar(df_testframe.groupby('State')['Weight (lbs)'].sum().reset_index(), x='State', y='Weight (lbs)', title='Total Weight by State')\n",
        "# Create a bar plot using Plotly Express for total weight by state\n",
        "# Arguments:\n",
        "# - df_testframe.groupby('State')['Weight (lbs)'].sum().reset_index(): Group by 'State' and sum 'Weight (lbs)', then reset index\n",
        "# - x='State': Specifies the x-axis variable (states)\n",
        "# - y='Weight (lbs)': Specifies the y-axis variable (total weight)\n",
        "# - title='Total Weight by State': Sets the title of the plot\n",
        "fig.show()  # Display the Plotly figure\n",
        "\n",
        "# Scatter Plot of Height vs. Weight\n",
        "fig = px.scatter(df_testframe, x='Height (ft)', y='Weight (lbs)', title='Height vs. Weight')\n",
        "# Create a scatter plot using Plotly Express for 'Height (ft)' vs. 'Weight (lbs)'\n",
        "# Arguments:\n",
        "# - x='Height (ft)': Specifies the x-axis variable (height)\n",
        "# - y='Weight (lbs)': Specifies the y-axis variable (weight)\n",
        "# - title='Height vs. Weight': Sets the title of the plot\n",
        "fig.show()  # Display the Plotly figure\n",
        "\n",
        "# Pie Chart of State Distribution\n",
        "fig = px.pie(df_testframe, names='State', title='State Distribution')\n",
        "# Create a pie chart using Plotly Express for the distribution of 'State'\n",
        "# Arguments:\n",
        "# - names='State': Specifies the column to use for pie chart categories\n",
        "# - title='State Distribution': Sets the title of the plot\n",
        "fig.show()  # Display the Plotly figure\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "emK0Mb-FY79K",
        "outputId": "c9ef0824-ed40-4512-cb91-abf6d34e229d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport plotly.express as px\\nimport plotly.graph_objects as go\\n\\n# Sample data\\ndata = {\\n    'Date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04'],\\n    'Name': ['John Doe', 'Jane Smith', 'Alice Johnson', 'Bob Brown'],\\n    'Height (ft)': [5.9, 5.5, 5.7, 6.0],\\n    'Weight (lbs)': [70.5, 65.2, 80.8, np.nan],  # Including a NaN value for demonstration\\n    'State': ['California', 'New York', 'Texas', 'Florida'],\\n    'City': ['Los Angeles', 'New York', 'Houston', 'Miami'],\\n    'LatLong': ['34.0522,-118.2437', '40.7128,-74.0060', '29.7604,-95.3698', '25.7617,-80.1918']\\n}\\n\\ndf_testframe = pd.DataFrame(data)\\n\\n# Matplotlib Visualizations\\n\\n# Bar Plot of Weight by State\\nplt.figure(figsize=(10, 6))\\ndf_testframe.groupby('State')['Weight (lbs)'].sum().plot(kind='bar', color='skyblue')\\nplt.title('Total Weight by State')\\nplt.xlabel('State')\\nplt.ylabel('Total Weight (lbs)')\\nplt.xticks(rotation=45)\\nplt.show()\\n\\n# Scatter Plot of Height vs. Weight\\nplt.figure(figsize=(10, 6))\\nplt.scatter(df_testframe['Height (ft)'], df_testframe['Weight (lbs)'], color='green')\\nplt.title('Height vs. Weight')\\nplt.xlabel('Height (ft)')\\nplt.ylabel('Weight (lbs)')\\nplt.show()\\n\\n# Pie Chart of State Distribution\\nplt.figure(figsize=(8, 8))\\ndf_testframe['State'].value_counts().plot(kind='pie', autopct='%1.1f%%', startangle=140, colors=['gold', 'yellowgreen', 'lightcoral', 'lightskyblue'])\\nplt.title('State Distribution')\\nplt.ylabel('')\\nplt.show()\\n\\n# Seaborn Visualizations\\n\\n# Box Plot of Weight by State\\nplt.figure(figsize=(10, 6))\\nsns.boxplot(x='State', y='Weight (lbs)', data=df_testframe)\\nplt.title('Weight Distribution by State')\\nplt.xlabel('State')\\nplt.ylabel('Weight (lbs)')\\nplt.xticks(rotation=45)\\nplt.show()\\n\\n# Violin Plot of Height by State\\nplt.figure(figsize=(10, 6))\\nsns.violinplot(x='State', y='Height (ft)', data=df_testframe)\\nplt.title('Height Distribution by State')\\nplt.xlabel('State')\\nplt.ylabel('Height (ft)')\\nplt.xticks(rotation=45)\\nplt.show()\\n\\n# Heatmap of Correlation Matrix (only for numeric columns)\\nplt.figure(figsize=(10, 6))\\nnumeric_columns = df_testframe.select_dtypes(include=[np.number]).columns\\ncorrelation_matrix = df_testframe[numeric_columns].corr()\\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\\nplt.title('Correlation Matrix')\\nplt.show()\\n\\n# Plotly Visualizations\\n\\n# Bar Plot of Weight by State\\nfig = px.bar(df_testframe.groupby('State')['Weight (lbs)'].sum().reset_index(), x='State', y='Weight (lbs)', title='Total Weight by State')\\nfig.show()\\n\\n# Scatter Plot of Height vs. Weight\\nfig = px.scatter(df_testframe, x='Height (ft)', y='Weight (lbs)', title='Height vs. Weight')\\nfig.show()\\n\\n# Pie Chart of State Distribution\\nfig = px.pie(df_testframe, names='State', title='State Distribution')\\nfig.show()\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Fuzzy Keyword-Matching rules and tagging to new KeyMatch column**"
      ],
      "metadata": {
        "id": "FMPITg9HqapB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#!pip install fuzzywuzzy\n",
        "#!pip install python-Levenshtein  # Optional, for better performance\n",
        "from fuzzywuzzy import fuzz\n",
        "from fuzzywuzzy import process\n",
        "import re\n",
        "\n",
        "\"\"\"\n",
        "If you prefer using rapidfuzz (recommended for speed):\n",
        "\n",
        "Replace from fuzzywuzzy import fuzz with from rapidfuzz.fuzz import partial_ratio.\n",
        "Install it using pip install rapidfuzz.\n",
        "\"\"\"\n",
        "\n",
        "# Example DataFrame named df_testframe\n",
        "\n",
        "df_testframe = pd.DataFrame(df_testframe)\n",
        "\n",
        "# Define your keywords\n",
        "keywords = [\"Texas\", \"One\", \"Three\", \"Four\", \"simple\"]\n",
        "\n",
        "#keywords = [\"ZZZZZZZZZZZZZZZ\"]\n",
        "\n",
        "# Function to perform fuzzy search and append matches\n",
        "def fuzzy_search_and_append(df, keywords, threshold=80):\n",
        "    exact_matches_found = False\n",
        "    fuzzy_matches_found = False\n",
        "\n",
        "    def custom_fuzzy_score(str1, str2):\n",
        "        return max(\n",
        "            fuzz.partial_ratio(str1, str2),\n",
        "            fuzz.token_sort_ratio(str1, str2),\n",
        "            fuzz.token_set_ratio(str1, str2)\n",
        "        )\n",
        "\n",
        "    def strip_special_chars(text):\n",
        "        return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "    def has_valid_context(keyword, cell_value):\n",
        "        # Check if the keyword is surrounded by at least 2 characters (excluding spaces and special characters)\n",
        "        pattern = re.compile(r'[a-zA-Z0-9]{2,}')\n",
        "        before = pattern.search(cell_value[:cell_value.find(keyword)])\n",
        "        after = pattern.search(cell_value[cell_value.find(keyword) + len(keyword):])\n",
        "        return not (before and after)\n",
        "\n",
        "    # Ensure KeyMatch and KeyMatch_Variance columns exist\n",
        "    if 'KeyMatch' not in df.columns:\n",
        "        df['KeyMatch'] = ''\n",
        "    if 'KeyMatch_Variance' not in df.columns:\n",
        "        df['KeyMatch_Variance'] = ''\n",
        "\n",
        "    previous_unique_matches_count = 0\n",
        "\n",
        "    while True:\n",
        "        new_exact_matches_found = False\n",
        "        new_fuzzy_matches_found = False\n",
        "        current_unique_matches_count = 0\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            exact_matches = set()\n",
        "            fuzzy_matches = set()\n",
        "            new_matches = set()  # Initialize to avoid uninitialized variable error\n",
        "            new_variances = set()  # Initialize to avoid uninitialized variable error\n",
        "\n",
        "            for col in df.columns:\n",
        "                if col not in ['KeyMatch', 'KeyMatch_Variance']:\n",
        "                    cell_value = str(row[col])\n",
        "                    for keyword in keywords:\n",
        "                        try:\n",
        "                            # Strip special characters and spaces\n",
        "                            stripped_keyword = strip_special_chars(keyword).replace(' ', '')\n",
        "                            stripped_cell_value = strip_special_chars(cell_value).replace(' ', '')\n",
        "\n",
        "                            # Calculate fuzzy score\n",
        "                            ratio = custom_fuzzy_score(stripped_keyword.lower(), stripped_cell_value.lower())\n",
        "                            # Adjust threshold based on keyword length\n",
        "                            adjusted_threshold = threshold - min(10, len(keyword) // 2)\n",
        "                            if ratio >= adjusted_threshold:\n",
        "                                # Check if the matched portion is an exact match\n",
        "                                matched_portion = process.extractOne(keyword, [cell_value], scorer=custom_fuzzy_score, score_cutoff=adjusted_threshold)[0]\n",
        "                                if matched_portion.lower() == keyword.lower():\n",
        "                                    exact_matches.add(keyword)\n",
        "                                else:\n",
        "                                    # Check if the matched portion contains special characters or spaces\n",
        "                                    if re.search(r'[^a-zA-Z0-9]', matched_portion) and len(matched_portion) > 2 and not matched_portion.isdigit() and has_valid_context(keyword, cell_value):\n",
        "                                        fuzzy_matches.add((matched_portion, keyword))\n",
        "                                    # Include the original cell value in KeyMatch_Variance if it contains special characters\n",
        "                                    if re.search(r'[^a-zA-Z0-9]', cell_value):\n",
        "                                        fuzzy_matches.add((cell_value, keyword))\n",
        "                        except Exception as e:\n",
        "                            # Ignore any errors during fuzzy search\n",
        "                            print(f\"Error processing {keyword} in {cell_value}: {e}\")\n",
        "\n",
        "            # Append exact matches to KeyMatch\n",
        "            if exact_matches:\n",
        "                new_exact_matches_found = True\n",
        "                existing_matches = set(row.get('KeyMatch', '').split('|'))\n",
        "                new_matches = exact_matches - existing_matches\n",
        "\n",
        "                if new_matches:\n",
        "                    if row.get('KeyMatch', ''):\n",
        "                        df.at[index, 'KeyMatch'] += '|' + '|'.join(new_matches)\n",
        "                    else:\n",
        "                        df.at[index, 'KeyMatch'] = '|'.join(new_matches)\n",
        "\n",
        "            # Append fuzzy matches to KeyMatch_Variance\n",
        "            if fuzzy_matches:\n",
        "                new_fuzzy_matches_found = True\n",
        "                existing_variances = set(row.get('KeyMatch_Variance', '').split('|'))\n",
        "                new_variances = {fuzzy[0] for fuzzy in fuzzy_matches if fuzzy[0] not in existing_variances}\n",
        "\n",
        "                if new_variances:\n",
        "                    if row.get('KeyMatch_Variance', ''):\n",
        "                        df.at[index, 'KeyMatch_Variance'] += '|' + '|'.join(new_variances)\n",
        "                    else:\n",
        "                        df.at[index, 'KeyMatch_Variance'] = '|'.join(new_variances)\n",
        "\n",
        "                # Append corresponding keywords to KeyMatch\n",
        "                corresponding_keywords = {fuzzy[1] for fuzzy in fuzzy_matches if fuzzy[1] not in exact_matches}\n",
        "                if corresponding_keywords:\n",
        "                    existing_matches = set(row.get('KeyMatch', '').split('|'))\n",
        "                    new_matches = corresponding_keywords - existing_matches\n",
        "\n",
        "                    if new_matches:\n",
        "                        if row.get('KeyMatch', ''):\n",
        "                            df.at[index, 'KeyMatch'] += '|' + '|'.join(new_matches)\n",
        "                        else:\n",
        "                            df.at[index, 'KeyMatch'] = '|'.join(new_matches)\n",
        "\n",
        "            # Count unique matches for this iteration\n",
        "            try:\n",
        "                current_unique_matches_count += len(new_matches) + len(new_variances)\n",
        "            except Exception as e:\n",
        "                print(f\"Error calculating unique matches count: {e}\")\n",
        "\n",
        "        # If no new matches are found, break the loop\n",
        "        if not new_exact_matches_found and not new_fuzzy_matches_found:\n",
        "            break\n",
        "\n",
        "        # Check if the number of unique matches has increased\n",
        "        if current_unique_matches_count <= previous_unique_matches_count:\n",
        "            break\n",
        "\n",
        "        # Update the previous unique matches count\n",
        "        previous_unique_matches_count = current_unique_matches_count\n",
        "\n",
        "\n",
        "\n",
        "    return df\n",
        "\n",
        "    # Ensure KeyMatch and KeyMatch_Variance columns exist if there are matches\n",
        "    if not exact_matches_found and 'KeyMatch' in df.columns:\n",
        "        df.drop(columns=['KeyMatch'], inplace=True)\n",
        "    if not fuzzy_matches_found and 'KeyMatch_Variance' in df.columns:\n",
        "        df.drop(columns=['KeyMatch_Variance'], inplace=True)\n",
        "\n",
        "\n",
        "# Perform fuzzy search and append matches\n",
        "df_testframe = fuzzy_search_and_append(df_testframe, keywords)\n",
        "\n",
        "print(df_testframe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3chmEV6bqbNk",
        "outputId": "6adee9d5-b166-4aa6-b561-8e5d67d6cffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date                 Name  Height (ft)  Weight (lbs)           City  \\\n",
            "0  2002-10-17        Vanessa Jones        77.00           217         Dallas   \n",
            "1  2024-10-29       Kristen Miller         4.56           155         Austin   \n",
            "2  2024-03-17        Shannon Boone         4.70           120       Santa Fe   \n",
            "3  2016-12-23      Spencer Edwards        54.00           173        Buffolo   \n",
            "4  2024-12-17       Donna Gonzalez         5.78           189    Albuquerque   \n",
            "5  2014-09-28       Cynthia Hebert        77.00           155        Buffalo   \n",
            "6  2024-09-04      Julie Hernandez         6.28           141       Santa Fe   \n",
            "7  2023-04-05        Bradley Lopez         4.97           147        Buffalo   \n",
            "8  2023-08-12         Robert Smith         4.55           163       Santa Fe   \n",
            "9  2023-08-16           Jane Smith         5.93           131  New York City   \n",
            "10 1991-05-30           Alex Smith        76.00           168        Trenton   \n",
            "11 2024-03-02   Christopher Harris         4.95           195      Las Vegas   \n",
            "12 2005-12-12         Steven Jones        56.00           209        Chicago   \n",
            "13 2019-02-09  Alexander Butler MD        61.00           177        Chicago   \n",
            "14 1972-08-20         Scott Potter        73.00           217    Springfield   \n",
            "15 2009-04-02     Michael Anderson        60.00           139        Atlanta   \n",
            "16 2024-03-23     Jonathan Anthony         5.98           148        Atlanta   \n",
            "17 1980-07-20   Christopher Lawson        55.00           163    Los Angeles   \n",
            "\n",
            "         State                  LatLong   KeyMatch KeyMatch_Variance  \n",
            "0        Texas     44.098017, 14.031824  One|Texas     Vanessa Jones  \n",
            "1       Tex,as   64.998524, -124.977125      Texas            Tex,as  \n",
            "2   New Mexico  -64.283114, -129.733085        One     Shannon Boone  \n",
            "3     New York    54.908244, -35.580677                               \n",
            "4   New Mexico   -79.931169, 115.848934                               \n",
            "5     New York     69.242968, 51.426673                               \n",
            "6   New Mexico    44.462486, -25.763822                               \n",
            "7     New York    17.353626, -41.483768                               \n",
            "8   New Mexico                      NaN                               \n",
            "9     New York   -69.380686, 138.523842                               \n",
            "10  New Jersey     45.158004, 96.695544                               \n",
            "11      Nevada    -51.727222, -0.276747                               \n",
            "12    Illinois    15.035926, -49.681280        One      Steven Jones  \n",
            "13    Illinois     12.092409, 78.449137                               \n",
            "14    Illinois    17.259917, -11.502040                               \n",
            "15     Georgia    -61.203240, 64.973237                               \n",
            "16     Georgia   -78.086051, 148.729005                               \n",
            "17  California     -1.167150, 92.081577                               \n"
          ]
        }
      ]
    }
  ]
}